## 总览

![20080319_96325f230767e6ac385beuTiIglWzzkf](C:\Users\10184\Desktop\20080319_96325f230767e6ac385beuTiIglWzzkf.png)

一图胜千言，自己看吧



## Ch2 存储管理

### 基本框架

使用多级目录进行索引，实现线性地址到物理地址的映射。多级的好处是可以节省页面表所占的内存空间，如何节省?通常虚存中有空洞（free但是不释放造成）,将其对应的目录项置为空，就能省去一大块空间。2.4版本的内存分配如图所示

![20081111_9477e6fe879a69d35b15pPat2RQUajuk](C:\Users\10184\Desktop\20081111_9477e6fe879a69d35b15pPat2RQUajuk.gif)

PMD PGD定义在pgtable-2level.h

```c
/*
 * traditional i386 two-level paging structure:
 */

#define PGDIR_SHIFT	22                //pgd下标起始地址
#define PTRS_PER_PGD	1024          //pgd表种的指针个数

/*
 * the i386 is two-level, so we don't really have any
 * PMD directory physically.
 */
#define PMD_SHIFT	22	             //和pgd起始地址相同
#define PTRS_PER_PMD	1            //pmd中只有一个表项

#define PTRS_PER_PTE	1024    
```

要想实现三层映射落实到二层映射，这里令pmd长度为0，表项数为1。实际上就是直接保持原值不变。

32位地址意味4G空间，实际上分为两块，0-3G为各个进程的用户空间，3-4G为内核的系统空间。从进程角度看，每个进程都有4G虚拟内存空间，其中3G为自己的用户空间，最高1G为所有进程与内核的共享空间。

定义在page.h

```c
#define __PAGE_OFFSET		(0xC0000000)

#define __pa(x)			((unsigned long)(x)-PAGE_OFFSET)        //转为物理地址
#define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))//转为虚拟地址


//processor.h

#define TASK_SIZE (PAGE_OFFSET)  //进程用户空间的上限就通过他来定义
```



### 地址映射的全过程

段映射

首先看段寄存器CS DS ES SS结构![image-20200319193916381](C:\Users\10184\AppData\Roaming\Typora\typora-user-images\image-20200319193916381.png)

Ti =0  GDT ; Ti =1 LDT

看看CS的定义

```c
#define start_thread(regs, new_eip, new_esp) do {		\
	__asm__("movl %0,%%fs ; movl %0,%%gs": :"r" (0));	\
	set_fs(USER_DS);					\
	regs->xds = __USER_DS;					\
	regs->xes = __USER_DS;					\
	regs->xss = __USER_DS;					\
	regs->xcs = __USER_CS;					\
	regs->eip = new_eip;					\
	regs->esp = new_esp;					\
} while (0)

//xds是ds的映像，其余类推
```

可以看到这里实际只有USER_DS  USER_CS两项，实际linux内核中ds es ss不分

```c
#define __KERNEL_CS	0x10
#define __KERNEL_DS	0x18

#define __USER_CS	0x23
#define __USER_DS	0x2B
```

将这几个按照二进制展开，对应段寄存器的图，可以发现一一对应，而且RPL也只用了0级和3级。

初始GDT内容定义

```c
ENTRY(gdt_table)
	.quad 0x0000000000000000	/* NULL descriptor */
	.quad 0x0000000000000000	/* not used */
	.quad 0x00cf9a000000ffff	/* 0x10 kernel 4GB code at 0x00000000 */
	.quad 0x00cf92000000ffff	/* 0x18 kernel 4GB data at 0x00000000 */
	.quad 0x00cffa000000ffff	/* 0x23 user   4GB code at 0x00000000 */
	.quad 0x00cff2000000ffff	/* 0x2b user   4GB data at 0x00000000 */
	.quad 0x0000000000000000	/* not used */
	.quad 0x0000000000000000	/* not used */
```

可以看到，第一项第二项都不用， 3 4 5 6项对应上面的段寄存器的四个值。



### 重要的数据结构

首先是PGD PMD PT

```c
//page.h
#if CONFIG_X86_PAE   //36位地址
typedef struct { unsigned long pte_low, pte_high; } pte_t;
typedef struct { unsigned long long pmd; } pmd_t;
typedef struct { unsigned long long pgd; } pgd_t;
#define pte_val(x)	((x).pte_low | ((unsigned long long)(x).pte_high << 32))
#else                //32位地址
typedef struct { unsigned long pte_low; } pte_t;
typedef struct { unsigned long pmd; } pmd_t;
typedef struct { unsigned long pgd; } pgd_t;
#define pte_val(x)	((x).pte_low)
#endif
#define PTE_MASK	PAGE_MASK

typedef struct { unsigned long pgprot; } pgprot_t; //页表项低12位，其中9位是标志位
```

![image-20200319200047311](C:\Users\10184\AppData\Roaming\Typora\typora-user-images\image-20200319200047311.png)

上图就是一个页表项

```c
//pgtable.h 
//对低12位的定义，图片对不到是内核版本问题
#define _PAGE_PRESENT	0x001  //存在位
#define _PAGE_RW	0x002
#define _PAGE_USER	0x004
#define _PAGE_PWT	0x008
#define _PAGE_PCD	0x010
#define _PAGE_ACCESSED	0x020
#define _PAGE_DIRTY	0x040
#define _PAGE_PSE	0x080	/* 4 MB (or 2MB) page, Pentium+, if present.. */
#define _PAGE_GLOBAL	0x100	/* Global TLB entry PPro+ */
```

如何得到一个完整地址？

```c
//pgtable.h

#define __mk_pte(page_nr,pgprot) __pte(((page_nr) << PAGE_SHIFT) | pgprot_val(pgprot)) 
//页面序号左移12位+低12位，__pte是转换成页表项的格式 

//page.h
#define pgprot_val(x)	((x).pgprot)
#define __pte(x) ((pte_t) { (x) } )
```



内核中有个全局量mem_map指针，指向page结构的数组，整个数组代表系统中全部的物理页面。

```c
//pgtable-2level.h
#define set_pte(pteptr, pteval) (*(pteptr) = pteval) //设置页表项

#define pte_none(x)		(!(x).pte_low) //页表项为0

//pgtable.h
#define pte_present(x)	((x).pte_low & (_PAGE_PRESENT | _PAGE_PROTNONE))
```

页表项为0说明尚未建立映射，存在位为0说明映射的物理页面不在内存中

```c
//pgtable.h

//下面存在的前提是存在位为1
static inline int pte_dirty(pte_t pte)		{ return (pte).pte_low & _PAGE_DIRTY; }
static inline int pte_young(pte_t pte)		{ return (pte).pte_low & _PAGE_ACCESSED; }
static inline int pte_write(pte_t pte)		{ return (pte).pte_low & _PAGE_RW; }
```

地址的高20位可以看成mem_map的下标，下面就是根据下标找到对应的page

```c
#define pte_page(pte)		\	 
			(mem_map + (unsigned long) ((pte_val(pte) & _PFN_MASK)>> PAGE_SHIFT))
```

下面是根据虚存找到物理的page结构

```c
#define virt_to_page(kaddr)	(mem_map + (__pa(kaddr) >> PAGE_SHIFT))
```



page结构定义如下

```c
typedef struct page {
	struct list_head list;
	struct address_space *mapping;
	unsigned long index; //页面在文件中的序号|页面去向
	struct page *next_hash;
	atomic_t count;
	unsigned long flags;	/* atomic flags, some possibly updated asynchronously */
	struct list_head lru;
	unsigned long age;
	wait_queue_head_t wait;
	struct page **pprev_hash;
	struct buffer_head * buffers;
	void *virtual; /* non-NULL if kmapped */
	struct zone_struct *zone;
} mem_map_t; //物理页帧
```

page又被划分到管理区ZONE_DMA,ZONE_NORMAL,ZONE_HIGHMEM。每个管理区都有一个数据结构zone_struct。

```c
typedef struct zone_struct {
	/*
	 * Commonly accessed fields:
	 */
	spinlock_t			lock;
	unsigned long		offset; // 表示当前区在mem_map中的起始页面号
	unsigned long		free_pages;
	unsigned long		inactive_clean_pages;
	unsigned long		inactive_dirty_pages;
	unsigned long		pages_min, pages_low, pages_high;

	/*
	 * free areas of different sizes
	 */
	struct list_head	inactive_clean_list;
	free_area_t			free_area[MAX_ORDER]; // 用于伙伴分配算法

	/*
	 * rarely used fields:
	 */
	char				*name;
	unsigned long		size;
	/*
	 * Discontig memory support fields.
	 */
	struct pglist_data	*zone_pgdat;  //指向所属的节点
	unsigned long		zone_start_paddr;
	unsigned long		zone_start_mapnr;
	struct page			*zone_mem_map;
} zone_t;
```

管理区又要被归类到更大的节点。节点是NUMA的概念，非均匀存储结构。于是在zone_struct基础上还有一层pglist_data。

```c
typedef struct pglist_data {
	zone_t node_zones[MAX_NR_ZONES];//三个管理区
	zonelist_t node_zonelists[NR_GFPINDEX];//分配策略
	struct page *node_mem_map;      //指向具体节点的page数组
	unsigned long *valid_addr_bitmap;
	struct bootmem_data *bdata;
	unsigned long node_start_paddr;
	unsigned long node_start_mapnr;
	unsigned long node_size;
	int node_id;
	struct pglist_data *node_next;   //形成单链队列
} pg_data_t;  //存储节点，用于NUMA

```

![image-20200319214204585](C:\Users\10184\AppData\Roaming\Typora\typora-user-images\image-20200319214204585.png)

再往下延申，pglist_data里有要给数组node_zonelists[],定义如下

```c
typedef struct zonelist_struct {
	zone_t * zones[MAX_NR_ZONES+1]; // 指针数组，各元素按特定次序指向具体的页面管理区
	int gfp_mask;
} zonelist_t;  //提供了分配策略

#define NR_GFPINDEX		0x100   //zonelist_t最多有100种分配策略
```



下面看虚拟空间管理，以进程为基础，而没有总的仓库这种概念。vm_area_struct是其中的重要结构。

```c
struct vm_area_struct {
	struct mm_struct * vm_mm;	/* VM area parameters */  // [start,end)
	unsigned long vm_start;
	unsigned long vm_end;

	/* linked list of VM areas per task, sorted by address */
	struct vm_area_struct *vm_next;  //区间按照地址顺序排列

    //权限属性，同一区间里面的页面都应该相同
	pgprot_t vm_page_prot;
	unsigned long vm_flags;

	/* AVL tree of VM areas per task, sorted by address */
    //AVL树搜索更快
	short vm_avl_height;
	struct vm_area_struct * vm_avl_left;
	struct vm_area_struct * vm_avl_right;

	/* For areas with an address space and backing store,
	 * one of the address_space->i_mmap{,shared} lists,
	 * for shm areas, the list of attaches, otherwise unused.
	 */
	struct vm_area_struct *vm_next_share;
	struct vm_area_struct **vm_pprev_share;

	struct vm_operations_struct * vm_ops;
	unsigned long vm_pgoff;		/* offset in PAGE_SIZE units, *not* PAGE_CACHE_SIZE */
	struct file * vm_file;
	unsigned long vm_raend;
	void * vm_private_data;		/* was vm_pte (shared mem) */
};
```

区间的划分不仅取决于地址的连续性，还取决于区间的其他属性。如果一个区间前一半和后一半的访问权限不同，也得分成两个部分。同一进程的所有区间都要按地址的高低连接在一起，用到vm_next。

两种情况下虚存会与磁盘文件交互。1.盘区交换，按需分配的页式虚拟内存管理 2.mmap将磁盘文件映射到内存的用户空间。

虚存空间另一个重要结构是上面的vm_ops

```c
//用于虚存区间的开关和建立映射
struct vm_operations_struct {
	void (*open)(struct vm_area_struct * area);
	void (*close)(struct vm_area_struct * area);
	struct page * (*nopage)(struct vm_area_struct * area, unsigned long address, int write_access);
};
//都是函数指针
```

还有一个指针vm_mm,指向mm_struct

```c
//sched.h

//每个进程只有一个
struct mm_struct {
	struct vm_area_struct * mmap;		/* list of VMAs */ 
	struct vm_area_struct * mmap_avl;	/* tree of VMAs */ /
	struct vm_area_struct * mmap_cache;	/* last find_vma result */ 
	pgd_t * pgd;			    
	atomic_t mm_users;			/* How many users with user space? */
	atomic_t mm_count;			//有几个虚存区间
	int map_count;				/* number of VMAs */
	struct semaphore mmap_sem;  
	spinlock_t page_table_lock;

	struct list_head mmlist;		/* List of all active mm's */

	unsigned long start_code, end_code, start_data, end_data;
	unsigned long start_brk, brk, start_stack;
	unsigned long arg_start, arg_end, env_start, env_end;
	unsigned long rss, total_vm, locked_vm;
	unsigned long def_flags;
	unsigned long cpu_vm_mask;
	unsigned long swap_cnt;	/* number of pages to swap on next pass */
	unsigned long swap_address;

	/* Architecture-specific MM context */
	mm_context_t context;
};
```

可以将mm_struct看作进程整个用户空间的抽象。虽然一个进程只有一个mm_struct，但是一个mm_struct可以被多个进程共享。

在内核中，给定一个属于某进程的虚拟地址，找到其所属的区间和对应的vm_area_struct很常见，使用find_vma来寻找。

```c
struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr)
{
	struct vm_area_struct *vma = NULL;

	if (mm) {
		/* Check the cache first. */
		/* (Cache hit rate is typically around 35%.) */
		vma = mm->mmap_cache;//首先看缓存是否命中
		if (!(vma && vma->vm_end > addr && vma->vm_start <= addr)) {
			if (!mm->mmap_avl) { //avl树为空，就遍历链表
				/* Go through the linear list. */
				vma = mm->mmap;
				while (vma && vma->vm_end <= addr)
					vma = vma->vm_next; //向后寻址
			} else {
				/* Then go through the AVL tree quickly. */
				struct vm_area_struct * tree = mm->mmap_avl;
				vma = NULL;
				for (;;) {
					if (tree == vm_avl_empty)
						break;
					if (tree->vm_end > addr) {
						vma = tree;
						if (tree->vm_start <= addr)
							break;
						tree = tree->vm_avl_left;
					} else
						tree = tree->vm_avl_right;
				}
			}
			if (vma)
				mm->mmap_cache = vma;//赋给缓存
		}
	}
	return vma;
}
```

如果函数返回0，说明地址尚未建立，那么就要新建虚存区进行插入。

```c
void insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vmp)
{
	lock_vma_mappings(vmp); //新区间的锁
	spin_lock(&current->mm->page_table_lock);//整个虚存空间的锁
	__insert_vm_struct(mm, vmp); //调用主体
	spin_unlock(&current->mm->page_table_lock);
	unlock_vma_mappings(vmp);
}

```

下面是insert_vm_struct的调用主体

```c
void __insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vmp)
{
	struct vm_area_struct **pprev;
	struct file * file;

	if (!mm->mmap_avl) {
		pprev = &mm->mmap;
		while (*pprev && (*pprev)->vm_start <= vmp->vm_start)
			pprev = &(*pprev)->vm_next;
	} else {
		struct vm_area_struct *prev, *next;
		avl_insert_neighbours(vmp, &mm->mmap_avl, &prev, &next);
		pprev = (prev ? &prev->vm_next : &mm->mmap);
		if (*pprev != next)
			printk("insert_vm_struct: tree inconsistent with list\n");
	}
	vmp->vm_next = *pprev;
	*pprev = vmp;

	mm->map_count++;
	if (mm->map_count >= AVL_MIN_MAP_COUNT && !mm->mmap_avl)
		build_mmap_avl(mm);

	file = vmp->vm_file;
	if (file) {
		struct inode * inode = file->f_dentry->d_inode;
		struct address_space *mapping = inode->i_mapping;
		struct vm_area_struct **head;

		if (vmp->vm_flags & VM_DENYWRITE)
			atomic_dec(&inode->i_writecount);

		head = &mapping->i_mmap;
		if (vmp->vm_flags & VM_SHARED)
			head = &mapping->i_mmap_shared;

		/* insert vmp into inode's share list */
		if((vmp->vm_next_share = *head) != NULL)
			(*head)->vm_pprev_share = &vmp->vm_next_share;
		*head = vmp;
		vmp->vm_pprev_share = head;
	}
}
```

上面的函数要分析完

